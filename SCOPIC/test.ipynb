{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install pytorch-lightning\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from transformers import AutoModel, AdamW, get_cosine_schedule_with_warmup\n",
    "# from torch.optim.AdamW import AdamW \n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchmetrics.functional.classification import auroc\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata():\n",
    "\n",
    "    df_user = pd.read_excel(\"data 1/CodeAid Source Codes Labeling.xlsx\")\n",
    "    df = pd.DataFrame(columns=['instruction','Code', '3.5 Turbo','GPT-4', '4 Turbo'])\n",
    "    \n",
    "    for x in range (0,63):\n",
    "        instructionstr = \"\"\n",
    "        location = f\"data 1/dataset-source-codes/source_code_00{str(x)}/\"\n",
    "        location_json = f\"data 1/dataset-source-codes/source_code_00{str(x)}/source_code_00{str(x)}.json\"\n",
    "        #This condition handles when the file name number has two digits instead of 1\n",
    "        if(x>9):\n",
    "            location = f\"data 1/dataset-source-codes/source_code_0{str(x)}/\"\n",
    "            location_json = f\"data 1/dataset-source-codes/source_code_0{str(x)}/source_code_0{str(x)}.json\"\n",
    "\n",
    "\n",
    "        with open(location_json, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if 'rules' in data and isinstance(data['rules'], list):\n",
    "            data['rules'] = (', '.join(data['rules']))\n",
    "            \n",
    "        instructionstr = (data[\"question\"] + \", \"+data['rules'] )\n",
    "        \n",
    "        for infile in glob.glob(os.path.join(location, '*')):\n",
    "            review_file = open(infile,'r').read()\n",
    "\n",
    "            if(infile.find(\"gpt-4_\")>0):\n",
    "                df.loc[len(df)] = [instructionstr,review_file,0,1,0]\n",
    "                continue\n",
    "\n",
    "            elif(infile.find(\"4-turbo\")>0):\n",
    "                df.loc[len(df)] = [instructionstr,review_file,0,0,1]\n",
    "                continue\n",
    "\n",
    "            elif(infile.find(\"3.5\")>0):\n",
    "                df.loc[len(df)] = [instructionstr,review_file,1,0,0]\n",
    "                continue\n",
    "\n",
    "            elif(infile.find(\"json\")<=0):\n",
    "                \n",
    "                concatarr = [0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "                for y in range(6):\n",
    "                    concatarr[y] = float(df_user['plagiarism_score'][(x*6)+y])\n",
    "                df.loc[len(df)] = [instructionstr,review_file,concatarr[0],concatarr[2],concatarr[4]]\n",
    "                df.loc[len(df)] = [instructionstr,review_file,concatarr[1],concatarr[3],concatarr[5]]\n",
    "                continue\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    df.to_csv(\"Dataset.csv\",index=False) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Code and Instruction Textual Data and Shuffle the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata():\n",
    "    df = loaddata()\n",
    "\n",
    "    df['Code']=df['Code'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
    "    df['Code']=df['Code'].apply(lambda x: x.lower())\n",
    "    df['Code']=df['Code'].apply(lambda x: re.sub('\\n',' ',x))\n",
    "    df['Code']=df['Code'].apply(lambda x: re.sub(' +',' ',x))\n",
    "\n",
    "    df['instruction']=df['instruction'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x))\n",
    "    df['instruction']=df['instruction'].apply(lambda x: x.lower())\n",
    "    df['instruction']=df['instruction'].apply(lambda x: re.sub('\\n',' ',x))\n",
    "    df['instruction']=df['instruction'].apply(lambda x: re.sub(' +',' ',x))\n",
    "\n",
    "    shuffled = df.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "    shuffled.head()\n",
    "\n",
    "\n",
    "    trainsplit = int(len(shuffled)*.7)\n",
    "    testsplit = int((len(shuffled) - trainsplit)/2)+trainsplit\n",
    "\n",
    "    train_data = shuffled.loc[0:trainsplit]\n",
    "    test_data = shuffled.loc[trainsplit+1:testsplit]\n",
    "    val_data = shuffled.loc[testsplit+1:]\n",
    "\n",
    "    test_data.reset_index(drop=True,inplace=True)\n",
    "    val_data.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "    return train_data,val_data,test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GGWP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data,test_data = getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributes = ['3.5 Turbo', 'GPT-4','4 Turbo']\n",
    "\n",
    "val_data[attributes].sum().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code_Dataset(Dataset):\n",
    "\n",
    "  def __init__(self, data, tokenizer, attributes, max_token_len: int = 512):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.attributes = attributes\n",
    "    self.max_token_len = max_token_len\n",
    "      \n",
    "  def __len__(self):\n",
    "      return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    item = self.data.iloc[index]\n",
    "    code_sample = str(item.Code)\n",
    "    attributes = torch.FloatTensor(item[self.attributes])\n",
    "    tokens = self.tokenizer.encode_plus(code_sample,\n",
    "                                        add_special_tokens=True,\n",
    "                                        return_tensors='pt',\n",
    "                                        truncation=True,\n",
    "                                        padding='max_length',\n",
    "                                        max_length=self.max_token_len,\n",
    "                                        return_attention_mask = True)\n",
    "    return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(), 'labels': attributes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model_name = 'roberta-base'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "Code_ds_train = Code_Dataset(train_data, tokenizer, attributes=attributes)\n",
    "Code_ds_val = Code_Dataset(val_data, tokenizer, attributes=attributes)\n",
    "Code_ds_test = Code_Dataset(test_data, tokenizer, attributes=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code_Data_Module(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_data, val_data, test_data, attributes, batch_size: int = 4, max_token_length: int = 512,  model_name='distilbert-base-uncased'):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.attributes = attributes\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_length = max_token_length\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"train\":\n",
    "            self.train_dataset = Code_Dataset(self.train_data, attributes=self.attributes, tokenizer=self.tokenizer)\n",
    "            self.val_dataset = Code_Dataset(self.val_data, attributes=self.attributes, tokenizer=self.tokenizer)\n",
    "        if stage == 'predict':\n",
    "            self.test_dataset = Code_Dataset(self.test_data, attributes=self.attributes, tokenizer=self.tokenizer)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, num_workers=4, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size = self.batch_size, num_workers=4, shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_data, batch_size = self.batch_size, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Code_dm = Code_Data_Module(train_data, val_data, test_data, attributes)\n",
    "Code_dm.setup(\"train\")\n",
    "Code_dm.train_dataloader()\n",
    "len(Code_dm.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code_Classifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.pretrained_model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
    "        self.hidden = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.pretrained_model.config.hidden_size)\n",
    "        self.classifier = torch.nn.Linear(self.pretrained_model.config.hidden_size, self.config['n_labels'])\n",
    "        torch.nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        self.loss_func = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # roberta layer\n",
    "        output = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = torch.mean(output.last_hidden_state, 1)\n",
    "        # final logits\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        pooled_output = self.hidden(pooled_output)\n",
    "        pooled_output = F.relu(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        # calculate loss\n",
    "        loss = 0\n",
    "        if labels is not None:\n",
    "            loss = self.loss_func(logits.view(-1, self.config['n_labels']), labels.view(-1, self.config['n_labels']))\n",
    "        return loss, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"train loss \", loss, prog_bar = True, logger=True)\n",
    "        return {\"loss\":loss, \"predictions\":outputs, \"labels\": batch[\"labels\"]}\n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        self.log(\"validation loss \", loss, prog_bar = True, logger=True)\n",
    "        return {\"val_loss\": loss, \"predictions\":outputs, \"labels\": batch[\"labels\"]}\n",
    "    \n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, outputs = self(**batch)\n",
    "        return outputs\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.config['lr'], weight_decay=self.config['weight_decay'])\n",
    "        total_steps = self.config['train_size']/self.config['batch_size']\n",
    "        warmup_steps = math.floor(total_steps * self.config['warmup'])\n",
    "        scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "        return [optimizer],[scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'n_labels': len(attributes),\n",
    "    'batch_size': 4,\n",
    "    'lr': 1.5e-6,\n",
    "    'warmup': 0.2, \n",
    "    'train_size': len(Code_dm.train_dataloader()),\n",
    "    'weight_decay': 0.001,\n",
    "    'n_epochs': 10\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\ncool\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "c:\\Users\\ncool\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name             | Type              | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | pretrained_model | DistilBertModel   | 66.4 M | eval \n",
      "1 | hidden           | Linear            | 590 K  | train\n",
      "2 | classifier       | Linear            | 2.3 K  | train\n",
      "3 | loss_func        | BCEWithLogitsLoss | 0      | train\n",
      "4 | dropout          | Dropout           | 0      | train\n",
      "---------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.823   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "92        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ncool\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "Code_dm = Code_Data_Module(train_data, val_data, test_data, attributes)\n",
    "Code_dm.setup(\"train\")\n",
    "\n",
    "model = Code_Classifier(config)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=config['n_epochs'], num_sanity_val_steps=50)\n",
    "trainer.fit(model, Code_dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
